{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ca9b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame after renaming: Index(['Image no.', 'Date', 'Time', 'Place', 'NowCastConc.', 'PM2.5',\n",
      "       'AQI Category', 'Raw Conc.', 'Conc. Unit'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thanmai/Desktop/DAQUIP/.venv/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "/Users/thanmai/Desktop/DAQUIP/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 25050.5762 - mse: 25050.5762 - val_loss: 25375.6699 - val_mse: 25375.6699\n",
      "Epoch 2/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 24094.7695 - mse: 24094.7695 - val_loss: 23235.3652 - val_mse: 23235.3652\n",
      "Epoch 3/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - loss: 20955.4766 - mse: 20955.4766 - val_loss: 17770.3066 - val_mse: 17770.3066\n",
      "Epoch 4/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 14899.0645 - mse: 14899.0645 - val_loss: 9024.4453 - val_mse: 9024.4453\n",
      "Epoch 5/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 6313.0298 - mse: 6313.0298 - val_loss: 2898.8701 - val_mse: 2898.8701\n",
      "Epoch 6/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 2196.0498 - mse: 2196.0498 - val_loss: 1573.8494 - val_mse: 1573.8494\n",
      "Epoch 7/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - loss: 1501.8947 - mse: 1501.8947 - val_loss: 1297.6997 - val_mse: 1297.6997\n",
      "Epoch 8/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 1323.8663 - mse: 1323.8663 - val_loss: 1188.2052 - val_mse: 1188.2052\n",
      "Epoch 9/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 1105.4071 - mse: 1105.4071 - val_loss: 1152.7487 - val_mse: 1152.7487\n",
      "Epoch 10/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 1044.8868 - mse: 1044.8868 - val_loss: 1083.2168 - val_mse: 1083.2168\n",
      "Epoch 11/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 1041.5243 - mse: 1041.5243 - val_loss: 1078.2792 - val_mse: 1078.2792\n",
      "Epoch 12/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 994.6059 - mse: 994.6059 - val_loss: 1016.9042 - val_mse: 1016.9042\n",
      "Epoch 13/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - loss: 972.7187 - mse: 972.7187 - val_loss: 1005.7776 - val_mse: 1005.7776\n",
      "Epoch 14/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 947.0287 - mse: 947.0287 - val_loss: 979.2459 - val_mse: 979.2459\n",
      "Epoch 15/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 1001.0484 - mse: 1001.0484 - val_loss: 967.8306 - val_mse: 967.8306\n",
      "Epoch 16/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 946.6241 - mse: 946.6241 - val_loss: 956.2096 - val_mse: 956.2096\n",
      "Epoch 17/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1034.4680 - mse: 1034.4680 - val_loss: 936.7762 - val_mse: 936.7762\n",
      "Epoch 18/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 923.0324 - mse: 923.0324 - val_loss: 939.2346 - val_mse: 939.2346\n",
      "Epoch 19/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 818.1682 - mse: 818.1682 - val_loss: 909.3771 - val_mse: 909.3771\n",
      "Epoch 20/20\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 958.9217 - mse: 958.9217 - val_loss: 895.7748 - val_mse: 895.7748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Model Mean Squared Error: 527.5411525809114\n",
      "Model R²: 0.8104992508888245\n",
      "Baseline MSE: 2785.1009443398357\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "# Load the data with the correct header\n",
    "labels = pd.read_excel('/Users/thanmai/Desktop/DAQUIP/dataset/labels.xlsx', header=1)\n",
    "\n",
    "# Rename the columns\n",
    "labels.columns = ['Image no.', 'Date', 'Time', 'Place', 'NowCastConc.', 'PM2.5', 'AQI Category', 'Raw Conc.', 'Conc. Unit']\n",
    "\n",
    "# Verify the column names\n",
    "print(\"Columns in the DataFrame after renaming:\", labels.columns)\n",
    "\n",
    "# Function to parse dates with multiple formats\n",
    "def parse_date(date_str):\n",
    "    if isinstance(date_str, datetime):\n",
    "        return date_str\n",
    "    for fmt in ('%d.%m.%Y', '%Y-%m-%d'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Date format for {date_str} not recognized\")\n",
    "\n",
    "# Convert Date\n",
    "labels['Date'] = labels['Date'].apply(parse_date)\n",
    "\n",
    "# Function to parse times with multiple formats\n",
    "def parse_time(time_str):\n",
    "    if isinstance(time_str, datetime):\n",
    "        return time_str\n",
    "    for fmt in ('%I.%M%p', '%I:%M:%S%p', '%H:%M:%S'):\n",
    "        try:\n",
    "            return datetime.strptime(time_str, fmt).time()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Time format for {time_str} not recognized\")\n",
    "\n",
    "# Ensure 'Time' column is converted to strings before parsing\n",
    "labels['Time'] = labels['Time'].astype(str)\n",
    "labels['Time'] = labels['Time'].apply(parse_time)\n",
    "\n",
    "# Extract useful features from Date and Time\n",
    "labels['Hour'] = labels['Time'].apply(lambda x: x.hour)\n",
    "labels['DayOfWeek'] = labels['Date'].apply(lambda x: x.weekday())\n",
    "labels['Month'] = labels['Date'].apply(lambda x: x.month)\n",
    "\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_place = encoder.fit_transform(labels[['Place']])\n",
    "encoded_aqi_category = encoder.fit_transform(labels[['AQI Category']])\n",
    "\n",
    "# Function definitions for feature extraction\n",
    "def extract_dark_channel(image):\n",
    "    min_channel = np.min(image, axis=2)\n",
    "    return min_channel\n",
    "\n",
    "def estimate_transmission(dark_channel, omega=0.95):\n",
    "    transmission = 1 - omega * dark_channel\n",
    "    return transmission\n",
    "\n",
    "def get_sky_color(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    sky_color = np.mean(lab, axis=(0, 1))\n",
    "    return sky_color\n",
    "\n",
    "def power_spectrum_slope(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    f = np.fft.fft2(gray)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(fshift))\n",
    "    slope = np.mean(magnitude_spectrum)\n",
    "    return slope\n",
    "\n",
    "def calculate_contrast(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    contrast = np.sqrt(np.mean((gray - np.mean(gray))**2))\n",
    "    return contrast\n",
    "\n",
    "def calculate_normalized_saturation(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    saturation = hsv[..., 1]\n",
    "    normalized_saturation = (saturation - np.min(saturation)) / (np.max(saturation) - np.min(saturation))\n",
    "    histogram = np.histogram(normalized_saturation, bins=10, range=(0, 1))[0]\n",
    "    return histogram\n",
    "\n",
    "# Define the function for feature extraction from images\n",
    "def extract_features(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Error reading image: {image_path}\")\n",
    "        \n",
    "        dark_channel = extract_dark_channel(image)\n",
    "        transmission = estimate_transmission(dark_channel)\n",
    "        sky_color = get_sky_color(image)\n",
    "        slope = power_spectrum_slope(image)\n",
    "        contrast = calculate_contrast(image)\n",
    "        normalized_saturation = calculate_normalized_saturation(image)\n",
    "\n",
    "        features = [\n",
    "            float(np.mean(dark_channel)),\n",
    "            float(np.mean(transmission)),\n",
    "            float(np.mean(sky_color)),\n",
    "            float(slope),\n",
    "            float(contrast),\n",
    "            *normalized_saturation.tolist()\n",
    "        ]\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [np.nan] * 7\n",
    "\n",
    "# Parallel feature extraction from images\n",
    "num_cores = -1  # Use all available CPU cores\n",
    "X_images = Parallel(n_jobs=num_cores)(delayed(extract_features)(f'/Users/thanmai/Desktop/DAQUIP/dataset/{row[\"Image no.\"]}.jpg') for index, row in labels.iterrows())\n",
    "\n",
    "# Convert extracted features to numpy array and handle NaNs\n",
    "X_images = np.array(X_images, dtype=np.float64)\n",
    "X_images = np.nan_to_num(X_images)  # Replace NaNs with 0s or other appropriate value\n",
    "\n",
    "# Combine all features\n",
    "X = np.hstack((X_images, encoded_place, encoded_aqi_category, labels[['NowCastConc.', 'Raw Conc.', 'Hour', 'DayOfWeek', 'Month']].values))\n",
    "y = labels['PM2.5'].values\n",
    "\n",
    "# Normalize/Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Ensure there are no NaNs in the combined dataset\n",
    "X = np.nan_to_num(X)\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# Save the model\n",
    "model.save('/Users/thanmai/Desktop/DAQUIP/models/cnn_model.h5')\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Model Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R² score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Model R²: {r2}\")\n",
    "\n",
    "# Calculate baseline model\n",
    "baseline_pred = np.mean(y_train)\n",
    "baseline_mse = mean_squared_error(y_test, [baseline_pred] * len(y_test))\n",
    "print(f\"Baseline MSE: {baseline_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a2ed880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Example of feature extraction from date and time\n",
    "labels['Day'] = labels['Date'].dt.day\n",
    "labels['Month'] = labels['Date'].dt.month\n",
    "labels['Hour'] = labels['Time'].dt.hour\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels['PM2.5'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Random Forest MSE: {mse}\")\n",
    "print(f\"Random Forest R²: {r2}\")\n",
    "\n",
    "# Ensemble with Gradient Boosting\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gbr = gbr.predict(X_test)\n",
    "mse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
    "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
    "print(f\"Gradient Boosting MSE: {mse_gbr}\")\n",
    "print(f\"Gradient Boosting R²: {r2_gbr}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
